import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import time
from datetime import datetime

# Scikit-learn imports
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,
    classification_report, roc_curve, auc, roc_auc_score,
    mean_squared_error, mean_absolute_error, r2_score
)

# ë¶„ë¥˜ ëª¨ë¸ë“¤
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier

# íšŒê·€ ëª¨ë¸ë“¤
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ëª¨ë¸ í•™ìŠµ",
    page_icon="ğŸ¤–",
    layout="wide"
)

st.title("ğŸ¤– Step 3: ëª¨ë¸ í•™ìŠµ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
st.markdown("### ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” AI ëª¨ë¸ ë§Œë“¤ê¸°")

# ë°ì´í„° ì²´í¬
if 'data' not in st.session_state or st.session_state.data is None:
    st.error("âš ï¸ ë¨¼ì € ë°ì´í„°ë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš”!")
    if st.button("ë°ì´í„° ì¤€ë¹„ í˜ì´ì§€ë¡œ ì´ë™"):
        st.switch_page("pages/1_ğŸ“Š_ë°ì´í„°_ì¤€ë¹„.py")
    st.stop()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'models' not in st.session_state:
    st.session_state.models = {}
if 'best_model' not in st.session_state:
    st.session_state.best_model = None
if 'training_history' not in st.session_state:
    st.session_state.training_history = []

df = st.session_state.data.copy()
target_col = st.session_state.target_column
problem_type = st.session_state.data_type

# êµìœ¡ ì½˜í…ì¸ 
with st.expander("ğŸ“š ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì´ë€?", expanded=True):
    st.markdown("""
    ### ğŸ¯ ëª¨ë¸ í•™ìŠµ (Model Training)
    
    > **"ëª¨ë¸ì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ ì°¾ì•„ë‚´ëŠ” ê³¼ì •ì„ ê±°ì³ ì˜ˆì¸¡ ëŠ¥ë ¥ì„ ê°–ì¶”ê²Œ ë©ë‹ˆë‹¤"**
    
    ### ğŸ“Š í•µì‹¬ ê°œë…ë“¤:
    
    **1. í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í• **
    - **í›ˆë ¨ ì„¸íŠ¸ (70%)**: ëª¨ë¸ì´ íŒ¨í„´ì„ í•™ìŠµ
    - **ê²€ì¦ ì„¸íŠ¸ (15%)**: í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •
    - **í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ (15%)**: ìµœì¢… ì„±ëŠ¥ í‰ê°€
    
    **2. ê³¼ì í•© vs ê³¼ì†Œì í•©**
    - **ê³¼ì í•©**: í›ˆë ¨ ë°ì´í„°ì— ë„ˆë¬´ ìµœì í™” â†’ ìƒˆ ë°ì´í„°ì—ì„œ ì„±ëŠ¥ ì €í•˜
    - **ê³¼ì†Œì í•©**: íŒ¨í„´ì„ ì œëŒ€ë¡œ í•™ìŠµ ëª»í•¨ â†’ ì „ë°˜ì ìœ¼ë¡œ ë‚®ì€ ì„±ëŠ¥
    
    **3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**
    - ëª¨ë¸ì˜ í•™ìŠµ ë°©ì‹ì„ ì œì–´í•˜ëŠ” ì„¤ì •ê°’
    - ìµœì ê°’ì„ ì°¾ìœ¼ë©´ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒ
    
    **4. êµì°¨ ê²€ì¦ (Cross-Validation)**
    - ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ë°©ì‹ìœ¼ë¡œ ë‚˜ëˆ„ì–´ í‰ê°€
    - ë” ì•ˆì •ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ ì¸¡ì •
    """)

st.divider()

# ë°ì´í„° ì¤€ë¹„
X = df.drop(columns=[target_col])
y = df[target_col]

# íƒ€ê²Ÿ ì¸ì½”ë”© (ë¶„ë¥˜ ë¬¸ì œì—ì„œ ë¬¸ìì—´ì¸ ê²½ìš°)
if problem_type == 'classification' and y.dtype == 'object':
    le = LabelEncoder()
    y = le.fit_transform(y)
    st.session_state.label_encoder = le

# ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
X = X[numeric_cols]

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, 
    stratify=y if problem_type == 'classification' else None
)

# ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# íƒ­ êµ¬ì„±
tab1, tab2, tab3, tab4 = st.tabs([
    "ğŸ¯ ë¹ ë¥¸ í•™ìŠµ", 
    "ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹", 
    "âš”ï¸ ëª¨ë¸ ë¹„êµ", 
    "ğŸ“Š ìƒì„¸ í‰ê°€"
])

# ëª¨ë¸ ì •ì˜
if problem_type == 'classification':
    MODELS = {
        'Logistic Regression': {
            'model': LogisticRegression(max_iter=1000),
            'params': {
                'C': [0.001, 0.01, 0.1, 1, 10, 100],
                'solver': ['lbfgs', 'liblinear']
            },
            'description': 'ì„ í˜• ê²°ì • ê²½ê³„ë¥¼ ì‚¬ìš©í•˜ëŠ” í™•ë¥  ê¸°ë°˜ ë¶„ë¥˜ê¸°'
        },
        'Decision Tree': {
            'model': DecisionTreeClassifier(),
            'params': {
                'max_depth': [3, 5, 7, 10, None],
                'min_samples_split': [2, 5, 10],
                'criterion': ['gini', 'entropy']
            },
            'description': 'if-then ê·œì¹™ ê¸°ë°˜ì˜ íŠ¸ë¦¬ êµ¬ì¡° ë¶„ë¥˜ê¸°'
        },
        'Random Forest': {
            'model': RandomForestClassifier(),
            'params': {
                'n_estimators': [50, 100, 200],
                'max_depth': [5, 10, None],
                'min_samples_split': [2, 5]
            },
            'description': 'ì—¬ëŸ¬ ê²°ì • íŠ¸ë¦¬ì˜ ì•™ìƒë¸”ë¡œ ê³¼ì í•©ì„ ë°©ì§€'
        },
        'SVM': {
            'model': SVC(probability=True),
            'params': {
                'C': [0.1, 1, 10],
                'kernel': ['linear', 'rbf'],
                'gamma': ['scale', 'auto']
            },
            'description': 'ê³ ì°¨ì› ê³µê°„ì—ì„œ ìµœì ì˜ ê²°ì • ê²½ê³„ë¥¼ ì°¾ëŠ” ëª¨ë¸'
        },
        'KNN': {
            'model': KNeighborsClassifier(),
            'params': {
                'n_neighbors': [3, 5, 7, 9],
                'weights': ['uniform', 'distance']
            },
            'description': 'ê°€ê¹Œìš´ ì´ì›ƒë“¤ì˜ íˆ¬í‘œë¡œ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸'
        },
        'Naive Bayes': {
            'model': GaussianNB(),
            'params': {},
            'description': 'ë² ì´ì¦ˆ ì •ë¦¬ ê¸°ë°˜ì˜ í™•ë¥ ì  ë¶„ë¥˜ê¸°'
        },
        'Neural Network': {
            'model': MLPClassifier(max_iter=1000),
            'params': {
                'hidden_layer_sizes': [(50,), (100,), (50, 50)],
                'activation': ['relu', 'tanh'],
                'learning_rate_init': [0.001, 0.01]
            },
            'description': 'ì¸ê³µ ì‹ ê²½ë§ ê¸°ë°˜ì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸'
        }
    }
else:  # íšŒê·€
    MODELS = {
        'Linear Regression': {
            'model': LinearRegression(),
            'params': {},
            'description': 'ê°€ì¥ ê¸°ë³¸ì ì¸ ì„ í˜• íšŒê·€ ëª¨ë¸'
        },
        'Ridge Regression': {
            'model': Ridge(),
            'params': {
                'alpha': [0.001, 0.01, 0.1, 1, 10, 100]
            },
            'description': 'L2 ì •ê·œí™”ë¥¼ ì ìš©í•œ ì„ í˜• íšŒê·€'
        },
        'Lasso Regression': {
            'model': Lasso(),
            'params': {
                'alpha': [0.001, 0.01, 0.1, 1]
            },
            'description': 'L1 ì •ê·œí™”ë¥¼ ì ìš©í•œ ì„ í˜• íšŒê·€ (íŠ¹ì„± ì„ íƒ íš¨ê³¼)'
        },
        'Decision Tree': {
            'model': DecisionTreeRegressor(),
            'params': {
                'max_depth': [3, 5, 7, 10, None],
                'min_samples_split': [2, 5, 10]
            },
            'description': 'íŠ¸ë¦¬ êµ¬ì¡° ê¸°ë°˜ì˜ íšŒê·€ ëª¨ë¸'
        },
        'Random Forest': {
            'model': RandomForestRegressor(),
            'params': {
                'n_estimators': [50, 100, 200],
                'max_depth': [5, 10, None]
            },
            'description': 'ì—¬ëŸ¬ íŠ¸ë¦¬ì˜ í‰ê· ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” ì•™ìƒë¸” ëª¨ë¸'
        },
        'SVR': {
            'model': SVR(),
            'params': {
                'C': [0.1, 1, 10],
                'kernel': ['linear', 'rbf']
            },
            'description': 'Support Vector Machineì˜ íšŒê·€ ë²„ì „'
        }
    }

with tab1:
    st.markdown("## ğŸ¯ ë¹ ë¥¸ ëª¨ë¸ í•™ìŠµ")
    st.markdown("ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë¹ ë¥´ê²Œ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ë´…ë‹ˆë‹¤.")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        selected_model = st.selectbox(
            "í•™ìŠµí•  ëª¨ë¸ ì„ íƒ",
            list(MODELS.keys()),
            help="ê° ëª¨ë¸ì˜ íŠ¹ì§•ì„ í™•ì¸í•˜ê³  ì„ íƒí•˜ì„¸ìš”"
        )
    
    with col2:
        cv_folds = st.number_input("êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜", 3, 10, 5)
    
    # ëª¨ë¸ ì„¤ëª…
    st.info(f"**{selected_model}**: {MODELS[selected_model]['description']}")
    
    # í•™ìŠµ ë²„íŠ¼
    if st.button("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘", type="primary"):
        with st.spinner(f"{selected_model} í•™ìŠµ ì¤‘..."):
            # í”„ë¡œê·¸ë ˆìŠ¤ ë°”
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
            model = MODELS[selected_model]['model']
            
            # êµì°¨ ê²€ì¦
            status_text.text("êµì°¨ ê²€ì¦ ìˆ˜í–‰ ì¤‘...")
            progress_bar.progress(30)
            
            if problem_type == 'classification':
                cv_scores = cross_val_score(model, X_train_scaled, y_train, 
                                           cv=cv_folds, scoring='accuracy')
            else:
                cv_scores = cross_val_score(model, X_train_scaled, y_train, 
                                           cv=cv_folds, scoring='r2')
            
            # ëª¨ë¸ í•™ìŠµ
            status_text.text("ëª¨ë¸ í•™ìŠµ ì¤‘...")
            progress_bar.progress(60)
            
            start_time = time.time()
            model.fit(X_train_scaled, y_train)
            training_time = time.time() - start_time
            
            # ì˜ˆì¸¡
            status_text.text("ì˜ˆì¸¡ ë° í‰ê°€ ì¤‘...")
            progress_bar.progress(90)
            
            y_pred = model.predict(X_test_scaled)
            
            # ê²°ê³¼ ì €ì¥
            st.session_state.models[selected_model] = {
                'model': model,
                'cv_scores': cv_scores,
                'y_pred': y_pred,
                'training_time': training_time
            }
            
            progress_bar.progress(100)
            status_text.text("í•™ìŠµ ì™„ë£Œ!")
            time.sleep(0.5)
            progress_bar.empty()
            status_text.empty()
        
        # ê²°ê³¼ í‘œì‹œ
        st.success(f"âœ… {selected_model} í•™ìŠµ ì™„ë£Œ!")
        
        col1, col2, col3, col4 = st.columns(4)
        
        if problem_type == 'classification':
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted')
            recall = recall_score(y_test, y_pred, average='weighted')
            f1 = f1_score(y_test, y_pred, average='weighted')
            
            col1.metric("ì •í™•ë„", f"{accuracy:.3f}")
            col2.metric("ì •ë°€ë„", f"{precision:.3f}")
            col3.metric("ì¬í˜„ìœ¨", f"{recall:.3f}")
            col4.metric("F1 Score", f"{f1:.3f}")
        else:
            mse = mean_squared_error(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            rmse = np.sqrt(mse)
            
            col1.metric("RÂ² Score", f"{r2:.3f}")
            col2.metric("RMSE", f"{rmse:.3f}")
            col3.metric("MAE", f"{mae:.3f}")
            col4.metric("í•™ìŠµ ì‹œê°„", f"{training_time:.2f}ì´ˆ")
        
        # êµì°¨ ê²€ì¦ ê²°ê³¼
        st.markdown("### ğŸ“Š êµì°¨ ê²€ì¦ ê²°ê³¼")
        
        cv_df = pd.DataFrame({
            'Fold': [f'Fold {i+1}' for i in range(len(cv_scores))],
            'Score': cv_scores
        })
        
        fig = px.bar(cv_df, x='Fold', y='Score', 
                     title=f"êµì°¨ ê²€ì¦ ì ìˆ˜ (í‰ê· : {cv_scores.mean():.3f} Â± {cv_scores.std():.3f})")
        st.plotly_chart(fig, use_container_width=True)

with tab2:
    st.markdown("## ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
    st.markdown("ìµœì ì˜ ëª¨ë¸ ì„±ëŠ¥ì„ ìœ„í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.")
    
    tuning_model = st.selectbox(
        "íŠœë‹í•  ëª¨ë¸ ì„ íƒ",
        [m for m in MODELS.keys() if MODELS[m]['params']],
        help="í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ” ëª¨ë¸ë§Œ í‘œì‹œë©ë‹ˆë‹¤"
    )
    
    tuning_method = st.radio(
        "íŠœë‹ ë°©ë²•",
        ["Grid Search", "Random Search", "ìˆ˜ë™ ì¡°ì •"]
    )
    
    if tuning_method == "ìˆ˜ë™ ì¡°ì •":
        st.markdown("### ìˆ˜ë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •")
        
        # íŒŒë¼ë¯¸í„°ë³„ ìœ„ì ¯ ìƒì„±
        params = {}
        model_params = MODELS[tuning_model]['params']
        
        if model_params:
            for param_name, param_values in model_params.items():
                if isinstance(param_values[0], (int, float)):
                    # ìˆ˜ì¹˜í˜• íŒŒë¼ë¯¸í„°
                    if len(param_values) > 1:
                        params[param_name] = st.select_slider(
                            f"{param_name}",
                            options=param_values,
                            value=param_values[len(param_values)//2]
                        )
                else:
                    # ë²”ì£¼í˜• íŒŒë¼ë¯¸í„°
                    params[param_name] = st.selectbox(
                        f"{param_name}",
                        param_values
                    )
            
            if st.button("ì„ íƒí•œ íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµ"):
                with st.spinner("ëª¨ë¸ í•™ìŠµ ì¤‘..."):
                    # ëª¨ë¸ ìƒì„±
                    model_class = type(MODELS[tuning_model]['model'])
                    model = model_class(**params)
                    
                    # í•™ìŠµ
                    model.fit(X_train_scaled, y_train)
                    y_pred = model.predict(X_test_scaled)
                    
                    # ê²°ê³¼ í‘œì‹œ
                    if problem_type == 'classification':
                        score = accuracy_score(y_test, y_pred)
                        st.success(f"âœ… ì •í™•ë„: {score:.3f}")
                    else:
                        score = r2_score(y_test, y_pred)
                        st.success(f"âœ… RÂ² Score: {score:.3f}")
                    
                    # ì €ì¥
                    st.session_state.models[f"{tuning_model}_manual"] = {
                        'model': model,
                        'params': params,
                        'score': score
                    }
    
    elif tuning_method == "Grid Search":
        st.markdown("### Grid Search í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
        st.info("ëª¨ë“  ê°€ëŠ¥í•œ ì¡°í•©ì„ ì²´ê³„ì ìœ¼ë¡œ íƒìƒ‰í•©ë‹ˆë‹¤.")
        
        # íƒìƒ‰í•  íŒŒë¼ë¯¸í„° ì„ íƒ
        param_grid = {}
        model_params = MODELS[tuning_model]['params']
        
        if model_params:
            st.markdown("#### íƒìƒ‰í•  íŒŒë¼ë¯¸í„° ë²”ìœ„ ì„¤ì •")
            
            for param_name, param_values in model_params.items():
                if st.checkbox(f"{param_name} íƒìƒ‰", value=True):
                    param_grid[param_name] = param_values
            
            if param_grid and st.button("Grid Search ì‹œì‘", type="primary"):
                with st.spinner(f"Grid Search ì§„í–‰ ì¤‘... (ì¡°í•© ìˆ˜: {np.prod([len(v) for v in param_grid.values()])})"):
                    # Grid Search ìˆ˜í–‰
                    model = MODELS[tuning_model]['model']
                    
                    grid_search = GridSearchCV(
                        model, param_grid, 
                        cv=5,
                        scoring='accuracy' if problem_type == 'classification' else 'r2',
                        n_jobs=-1
                    )
                    
                    grid_search.fit(X_train_scaled, y_train)
                    
                    # ê²°ê³¼ ì €ì¥
                    best_model = grid_search.best_estimator_
                    best_params = grid_search.best_params_
                    best_score = grid_search.best_score_
                    
                    st.session_state.best_model = best_model
                    st.session_state.models[f"{tuning_model}_grid"] = {
                        'model': best_model,
                        'params': best_params,
                        'score': best_score,
                        'cv_results': grid_search.cv_results_
                    }
                
                # ê²°ê³¼ í‘œì‹œ
                st.success(f"âœ… ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("#### ìµœì  íŒŒë¼ë¯¸í„°")
                    for param, value in best_params.items():
                        st.write(f"**{param}**: {value}")
                
                with col2:
                    st.markdown("#### ì„±ëŠ¥")
                    st.metric("ìµœê³  CV ì ìˆ˜", f"{best_score:.3f}")
                    
                    # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€
                    y_pred = best_model.predict(X_test_scaled)
                    if problem_type == 'classification':
                        test_score = accuracy_score(y_test, y_pred)
                        st.metric("í…ŒìŠ¤íŠ¸ ì •í™•ë„", f"{test_score:.3f}")
                    else:
                        test_score = r2_score(y_test, y_pred)
                        st.metric("í…ŒìŠ¤íŠ¸ RÂ²", f"{test_score:.3f}")
                
                # Grid Search ê²°ê³¼ ì‹œê°í™”
                st.markdown("### ğŸ“Š Grid Search ê²°ê³¼ ë¶„ì„")
                
                results_df = pd.DataFrame(grid_search.cv_results_)
                
                # íŒŒë¼ë¯¸í„°ë³„ ì„±ëŠ¥ íˆíŠ¸ë§µ (2ê°œ íŒŒë¼ë¯¸í„°ì¸ ê²½ìš°)
                if len(param_grid) == 2:
                    param_names = list(param_grid.keys())
                    pivot_table = results_df.pivot_table(
                        values='mean_test_score',
                        index=f'param_{param_names[0]}',
                        columns=f'param_{param_names[1]}'
                    )
                    
                    fig = px.imshow(
                        pivot_table,
                        labels=dict(x=param_names[1], y=param_names[0], color="Score"),
                        title="íŒŒë¼ë¯¸í„° ì¡°í•©ë³„ ì„±ëŠ¥ íˆíŠ¸ë§µ"
                    )
                    st.plotly_chart(fig, use_container_width=True)
    
    else:  # Random Search
        st.markdown("### Random Search í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
        st.info("ë¬´ì‘ìœ„ë¡œ ì„ íƒí•œ ì¡°í•©ì„ íƒìƒ‰í•©ë‹ˆë‹¤. Grid Searchë³´ë‹¤ ë¹ ë¦…ë‹ˆë‹¤.")
        
        n_iter = st.slider("íƒìƒ‰í•  ì¡°í•© ìˆ˜", 10, 100, 20)
        
        if MODELS[tuning_model]['params'] and st.button("Random Search ì‹œì‘", type="primary"):
            with st.spinner(f"Random Search ì§„í–‰ ì¤‘... ({n_iter}ê°œ ì¡°í•©)"):
                model = MODELS[tuning_model]['model']
                param_distributions = MODELS[tuning_model]['params']
                
                random_search = RandomizedSearchCV(
                    model, param_distributions,
                    n_iter=n_iter,
                    cv=5,
                    scoring='accuracy' if problem_type == 'classification' else 'r2',
                    n_jobs=-1,
                    random_state=42
                )
                
                random_search.fit(X_train_scaled, y_train)
                
                # ê²°ê³¼ ì €ì¥
                best_model = random_search.best_estimator_
                best_params = random_search.best_params_
                best_score = random_search.best_score_
                
                st.session_state.models[f"{tuning_model}_random"] = {
                    'model': best_model,
                    'params': best_params,
                    'score': best_score
                }
            
            st.success(f"âœ… Random Search ì™„ë£Œ!")
            st.write("**ìµœì  íŒŒë¼ë¯¸í„°:**", best_params)
            st.metric("ìµœê³  CV ì ìˆ˜", f"{best_score:.3f}")

with tab3:
    st.markdown("## âš”ï¸ ëª¨ë¸ ë¹„êµ")
    st.markdown("ì—¬ëŸ¬ ëª¨ë¸ì„ ë™ì‹œì— í•™ìŠµì‹œì¼œ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.")
    
    # ë¹„êµí•  ëª¨ë¸ ì„ íƒ
    models_to_compare = st.multiselect(
        "ë¹„êµí•  ëª¨ë¸ë“¤ì„ ì„ íƒí•˜ì„¸ìš”",
        list(MODELS.keys()),
        default=list(MODELS.keys())[:3]
    )
    
    if st.button("ğŸ ëª¨ë¸ ë¹„êµ ì‹œì‘", type="primary") and models_to_compare:
        comparison_results = []
        
        # í”„ë¡œê·¸ë ˆìŠ¤ ë°”
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, model_name in enumerate(models_to_compare):
            status_text.text(f"{model_name} í•™ìŠµ ì¤‘...")
            progress_bar.progress((i + 1) / len(models_to_compare))
            
            # ëª¨ë¸ í•™ìŠµ
            model = MODELS[model_name]['model']
            
            start_time = time.time()
            model.fit(X_train_scaled, y_train)
            training_time = time.time() - start_time
            
            # ì˜ˆì¸¡
            y_pred = model.predict(X_test_scaled)
            
            # í‰ê°€
            if problem_type == 'classification':
                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred, average='weighted')
                recall = recall_score(y_test, y_pred, average='weighted')
                f1 = f1_score(y_test, y_pred, average='weighted')
                
                comparison_results.append({
                    'Model': model_name,
                    'Accuracy': accuracy,
                    'Precision': precision,
                    'Recall': recall,
                    'F1 Score': f1,
                    'Training Time (s)': training_time
                })
            else:
                r2 = r2_score(y_test, y_pred)
                mse = mean_squared_error(y_test, y_pred)
                mae = mean_absolute_error(y_test, y_pred)
                
                comparison_results.append({
                    'Model': model_name,
                    'RÂ² Score': r2,
                    'RMSE': np.sqrt(mse),
                    'MAE': mae,
                    'Training Time (s)': training_time
                })
        
        progress_bar.empty()
        status_text.empty()
        
        # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
        comparison_df = pd.DataFrame(comparison_results)
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        if problem_type == 'classification':
            best_model_name = comparison_df.loc[comparison_df['Accuracy'].idxmax(), 'Model']
            st.success(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: **{best_model_name}**")
        else:
            best_model_name = comparison_df.loc[comparison_df['RÂ² Score'].idxmax(), 'Model']
            st.success(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: **{best_model_name}**")
        
        # ê²°ê³¼ í…Œì´ë¸”
        st.markdown("### ğŸ“Š ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸”")
        
        # ìŠ¤íƒ€ì¼ë§ëœ ë°ì´í„°í”„ë ˆì„
        styled_df = comparison_df.style.highlight_max(
            axis=0, 
            subset=[col for col in comparison_df.columns if col not in ['Model', 'Training Time (s)']],
            color='lightgreen'
        ).highlight_min(
            axis=0,
            subset=['Training Time (s)'],
            color='lightblue'
        ).format({col: '{:.3f}' for col in comparison_df.columns if col != 'Model'})
        
        st.dataframe(styled_df, use_container_width=True)
        
        # ë ˆì´ë” ì°¨íŠ¸ (ë¶„ë¥˜ ë¬¸ì œ)
        if problem_type == 'classification':
            st.markdown("### ğŸ“ˆ ì„±ëŠ¥ ë ˆì´ë” ì°¨íŠ¸")
            
            fig = go.Figure()
            
            categories = ['Accuracy', 'Precision', 'Recall', 'F1 Score']
            
            for _, row in comparison_df.iterrows():
                fig.add_trace(go.Scatterpolar(
                    r=[row[cat] for cat in categories],
                    theta=categories,
                    fill='toself',
                    name=row['Model']
                ))
            
            fig.update_layout(
                polar=dict(
                    radialaxis=dict(
                        visible=True,
                        range=[0, 1]
                    )),
                showlegend=True,
                title="ëª¨ë¸ë³„ ì„±ëŠ¥ ì§€í‘œ ë¹„êµ"
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        # ë§‰ëŒ€ ê·¸ë˜í”„
        st.markdown("### ğŸ“Š ì„±ëŠ¥ ë§‰ëŒ€ ê·¸ë˜í”„")
        
        if problem_type == 'classification':
            metric_to_plot = st.selectbox("í‘œì‹œí•  ì§€í‘œ", categories)
        else:
            metric_to_plot = st.selectbox("í‘œì‹œí•  ì§€í‘œ", ['RÂ² Score', 'RMSE', 'MAE'])
        
        fig = px.bar(
            comparison_df,
            x='Model',
            y=metric_to_plot,
            title=f"{metric_to_plot} ë¹„êµ",
            color=metric_to_plot,
            color_continuous_scale='Blues'
        )
        st.plotly_chart(fig, use_container_width=True)

with tab4:
    st.markdown("## ğŸ“Š ìƒì„¸ ëª¨ë¸ í‰ê°€")
    
    # í‰ê°€í•  ëª¨ë¸ ì„ íƒ
    trained_models = list(st.session_state.models.keys())
    
    if trained_models:
        eval_model_name = st.selectbox(
            "í‰ê°€í•  ëª¨ë¸ ì„ íƒ",
            trained_models
        )
        
        if eval_model_name:
            model_info = st.session_state.models[eval_model_name]
            model = model_info['model']
            
            # ì˜ˆì¸¡
            y_pred = model.predict(X_test_scaled)
            
            if problem_type == 'classification':
                st.markdown("### ë¶„ë¥˜ ëª¨ë¸ ìƒì„¸ í‰ê°€")
                
                # Confusion Matrix
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("#### Confusion Matrix")
                    cm = confusion_matrix(y_test, y_pred)
                    
                    fig = px.imshow(
                        cm,
                        labels=dict(x="ì˜ˆì¸¡", y="ì‹¤ì œ", color="ê°œìˆ˜"),
                        text_auto=True,
                        color_continuous_scale='Blues'
                    )
                    fig.update_layout(title="í˜¼ë™ í–‰ë ¬")
                    st.plotly_chart(fig, use_container_width=True)
                
                with col2:
                    st.markdown("#### ì •ê·œí™”ëœ Confusion Matrix")
                    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
                    
                    fig = px.imshow(
                        cm_normalized,
                        labels=dict(x="ì˜ˆì¸¡", y="ì‹¤ì œ", color="ë¹„ìœ¨"),
                        text_auto='.2f',
                        color_continuous_scale='RdBu_r'
                    )
                    fig.update_layout(title="ì •ê·œí™”ëœ í˜¼ë™ í–‰ë ¬")
                    st.plotly_chart(fig, use_container_width=True)
                
                # Classification Report
                st.markdown("#### ğŸ“‹ ë¶„ë¥˜ ë¦¬í¬íŠ¸")
                
                if hasattr(st.session_state, 'label_encoder'):
                    target_names = st.session_state.label_encoder.classes_
                else:
                    target_names = [str(i) for i in np.unique(y_test)]
                
                report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)
                report_df = pd.DataFrame(report).transpose()
                
                st.dataframe(
                    report_df.style.format({'precision': '{:.3f}', 'recall': '{:.3f}', 
                                           'f1-score': '{:.3f}', 'support': '{:.0f}'}),
                    use_container_width=True
                )
                
                # ROC Curve (ì´ì§„ ë¶„ë¥˜)
                if len(np.unique(y_test)) == 2:
                    st.markdown("#### ROC Curve")
                    
                    if hasattr(model, 'predict_proba'):
                        y_proba = model.predict_proba(X_test_scaled)[:, 1]
                        fpr, tpr, _ = roc_curve(y_test, y_proba)
                        roc_auc = auc(fpr, tpr)
                        
                        fig = go.Figure()
                        fig.add_trace(go.Scatter(
                            x=fpr, y=tpr,
                            mode='lines',
                            name=f'ROC Curve (AUC = {roc_auc:.3f})',
                            line=dict(color='darkblue', width=2)
                        ))
                        fig.add_trace(go.Scatter(
                            x=[0, 1], y=[0, 1],
                            mode='lines',
                            name='Random Classifier',
                            line=dict(color='red', width=1, dash='dash')
                        ))
                        fig.update_layout(
                            title='ROC Curve',
                            xaxis_title='False Positive Rate',
                            yaxis_title='True Positive Rate',
                            showlegend=True
                        )
                        st.plotly_chart(fig, use_container_width=True)
                        
                        st.info(f"""
                        ğŸ’¡ **ROC-AUC í•´ì„:**
                        - AUC = {roc_auc:.3f}
                        - 1.0: ì™„ë²½í•œ ë¶„ë¥˜ê¸°
                        - 0.5: ë¬´ì‘ìœ„ ë¶„ë¥˜ê¸°
                        - 0.7-0.8: ì ì ˆí•œ ì„±ëŠ¥
                        - 0.8-0.9: ì¢‹ì€ ì„±ëŠ¥
                        - 0.9+: ë§¤ìš° ì¢‹ì€ ì„±ëŠ¥
                        """)
            
            else:  # íšŒê·€ ëª¨ë¸ í‰ê°€
                st.markdown("### íšŒê·€ ëª¨ë¸ ìƒì„¸ í‰ê°€")
                
                # ì˜ˆì¸¡ vs ì‹¤ì œ í”Œë¡¯
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("#### ì˜ˆì¸¡ vs ì‹¤ì œ")
                    
                    fig = go.Figure()
                    fig.add_trace(go.Scatter(
                        x=y_test, y=y_pred,
                        mode='markers',
                        name='ì˜ˆì¸¡ê°’',
                        marker=dict(color='blue', opacity=0.5)
                    ))
                    
                    # ì™„ë²½í•œ ì˜ˆì¸¡ ì„ 
                    min_val = min(y_test.min(), y_pred.min())
                    max_val = max(y_test.max(), y_pred.max())
                    fig.add_trace(go.Scatter(
                        x=[min_val, max_val],
                        y=[min_val, max_val],
                        mode='lines',
                        name='ì™„ë²½í•œ ì˜ˆì¸¡',
                        line=dict(color='red', dash='dash')
                    ))
                    
                    fig.update_layout(
                        title='ì˜ˆì¸¡ê°’ vs ì‹¤ì œê°’',
                        xaxis_title='ì‹¤ì œê°’',
                        yaxis_title='ì˜ˆì¸¡ê°’'
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                with col2:
                    st.markdown("#### ì”ì°¨ í”Œë¡¯")
                    
                    residuals = y_test - y_pred
                    
                    fig = go.Figure()
                    fig.add_trace(go.Scatter(
                        x=y_pred, y=residuals,
                        mode='markers',
                        name='ì”ì°¨',
                        marker=dict(color='green', opacity=0.5)
                    ))
                    fig.add_hline(y=0, line_dash="dash", line_color="red")
                    
                    fig.update_layout(
                        title='ì”ì°¨ í”Œë¡¯',
                        xaxis_title='ì˜ˆì¸¡ê°’',
                        yaxis_title='ì”ì°¨ (ì‹¤ì œ - ì˜ˆì¸¡)'
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                # ì”ì°¨ ë¶„í¬
                st.markdown("#### ì”ì°¨ ë¶„í¬ ë¶„ì„")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    # íˆìŠ¤í† ê·¸ë¨
                    fig = px.histogram(
                        residuals,
                        nbins=30,
                        title="ì”ì°¨ íˆìŠ¤í† ê·¸ë¨"
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                with col2:
                    # Q-Q plot
                    from scipy import stats
                    
                    fig = go.Figure()
                    
                    theoretical_quantiles = stats.norm.ppf(np.linspace(0.01, 0.99, len(residuals)))
                    sample_quantiles = np.sort(residuals)
                    
                    fig.add_trace(go.Scatter(
                        x=theoretical_quantiles,
                        y=sample_quantiles,
                        mode='markers',
                        name='ì”ì°¨ Q-Q'
                    ))
                    
                    min_val = min(theoretical_quantiles.min(), sample_quantiles.min())
                    max_val = max(theoretical_quantiles.max(), sample_quantiles.max())
                    fig.add_trace(go.Scatter(
                        x=[min_val, max_val],
                        y=[min_val, max_val],
                        mode='lines',
                        name='ì •ê·œë¶„í¬',
                        line=dict(color='red', dash='dash')
                    ))
                    
                    fig.update_layout(
                        title="ì”ì°¨ Q-Q Plot",
                        xaxis_title="ì´ë¡ ì  ë¶„ìœ„ìˆ˜",
                        yaxis_title="ìƒ˜í”Œ ë¶„ìœ„ìˆ˜"
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                # í‰ê°€ ë©”íŠ¸ë¦­
                st.markdown("#### ğŸ“Š íšŒê·€ í‰ê°€ ì§€í‘œ")
                
                metrics_col1, metrics_col2, metrics_col3, metrics_col4 = st.columns(4)
                
                r2 = r2_score(y_test, y_pred)
                mse = mean_squared_error(y_test, y_pred)
                mae = mean_absolute_error(y_test, y_pred)
                mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
                
                metrics_col1.metric("RÂ² Score", f"{r2:.4f}")
                metrics_col2.metric("RMSE", f"{np.sqrt(mse):.4f}")
                metrics_col3.metric("MAE", f"{mae:.4f}")
                metrics_col4.metric("MAPE", f"{mape:.2f}%")
                
                with st.expander("ğŸ“š í‰ê°€ ì§€í‘œ ì„¤ëª…"):
                    st.markdown("""
                    - **RÂ² Score**: ëª¨ë¸ì´ ì„¤ëª…í•˜ëŠ” ë¶„ì‚°ì˜ ë¹„ìœ¨ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)
                    - **RMSE**: í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ, í° ì˜¤ì°¨ì— ë¯¼ê°)
                    - **MAE**: í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ, í•´ì„ì´ ì‰¬ì›€)
                    - **MAPE**: í‰ê·  ì ˆëŒ€ ë°±ë¶„ìœ¨ ì˜¤ì°¨ (ìŠ¤ì¼€ì¼ì— ë¬´ê´€í•œ ë¹„êµ)
                    """)
            
            # íŠ¹ì„± ì¤‘ìš”ë„ (ê°€ëŠ¥í•œ ê²½ìš°)
            if hasattr(model, 'feature_importances_'):
                st.markdown("### ğŸ¯ íŠ¹ì„± ì¤‘ìš”ë„")
                
                feature_importance = pd.DataFrame({
                    'feature': X.columns,
                    'importance': model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                fig = px.bar(
                    feature_importance.head(10),
                    x='importance',
                    y='feature',
                    orientation='h',
                    title="Top 10 ì¤‘ìš” íŠ¹ì„±"
                )
                st.plotly_chart(fig, use_container_width=True)
                
                st.info("""
                ğŸ’¡ **íŠ¹ì„± ì¤‘ìš”ë„ í™œìš©ë²•:**
                - ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì„±ë“¤ì— ì§‘ì¤‘
                - ë¶ˆí•„ìš”í•œ íŠ¹ì„± ì œê±° ê³ ë ¤
                - ë„ë©”ì¸ ì§€ì‹ê³¼ ë¹„êµí•˜ì—¬ ê²€ì¦
                """)
    else:
        st.info("ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")

# ëª¨ë¸ ì €ì¥ ì„¹ì…˜
st.divider()
st.markdown("## ğŸ’¾ ëª¨ë¸ ì €ì¥ ë° ë°°í¬")

if st.session_state.models:
    col1, col2 = st.columns(2)
    
    with col1:
        save_model = st.selectbox(
            "ì €ì¥í•  ëª¨ë¸ ì„ íƒ",
            list(st.session_state.models.keys())
        )
        
        if st.button("ëª¨ë¸ ì €ì¥", type="primary"):
            import pickle
            
            model_to_save = st.session_state.models[save_model]['model']
            
            # ëª¨ë¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
            model_package = {
                'model': model_to_save,
                'scaler': scaler,
                'features': list(X.columns),
                'problem_type': problem_type,
                'model_name': save_model,
                'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S")
            }
            
            # pickleë¡œ ì €ì¥
            model_bytes = pickle.dumps(model_package)
            
            st.download_button(
                label="ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (.pkl)",
                data=model_bytes,
                file_name=f"model_{save_model}_{model_package['timestamp']}.pkl",
                mime="application/octet-stream"
            )
            
            st.success("âœ… ëª¨ë¸ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    with col2:
        st.markdown("### ğŸ“ ëª¨ë¸ ì‚¬ìš© ì½”ë“œ")
        
        code = f"""
# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
import pickle
import pandas as pd

with open('model_{save_model}.pkl', 'rb') as f:
    model_package = pickle.load(f)

model = model_package['model']
scaler = model_package['scaler']
features = model_package['features']

# ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ì˜ˆì¸¡
new_data = pd.DataFrame({{
    # íŠ¹ì„± ê°’ ì…ë ¥
    {', '.join([f"'{col}': [ê°’]" for col in X.columns[:3]])}
}})

# ìŠ¤ì¼€ì¼ë§
new_data_scaled = scaler.transform(new_data[features])

# ì˜ˆì¸¡
prediction = model.predict(new_data_scaled)
print(f"ì˜ˆì¸¡ ê²°ê³¼: {{prediction[0]}}")
"""
        st.code(code, language='python')

# ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
st.divider()
st.markdown("## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„")

col1, col2, col3 = st.columns(3)

with col2:
    if st.session_state.models:
        st.success("""
        ### âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!
        
        ì´ì œ ë‹¤ìŒì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
        - ëª¨ë¸ ì„±ëŠ¥ ì‹¬í™” ë¶„ì„
        - í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ê°€ íŠœë‹
        - ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ì˜ˆì¸¡
        - ëª¨ë¸ ë°°í¬ ì¤€ë¹„
        """)
        
        if st.button("ğŸ“ˆ ì‹¬í™” ë¶„ì„ í˜ì´ì§€ë¡œ", type="primary", use_container_width=True):
            st.switch_page("pages/4_ğŸ“ˆ_ì‹¬í™”_ë¶„ì„.py")
    else:
        st.info("ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ì„±ëŠ¥ì„ í™•ì¸í•´ë³´ì„¸ìš”!")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.markdown("### ğŸ¤– í•™ìŠµëœ ëª¨ë¸ë“¤")
    
    if st.session_state.models:
        for model_name, model_info in st.session_state.models.items():
            with st.expander(model_name):
                if 'score' in model_info:
                    st.write(f"ì ìˆ˜: {model_info['score']:.3f}")
                if 'params' in model_info:
                    st.write("íŒŒë¼ë¯¸í„°:", model_info['params'])
                if 'training_time' in model_info:
                    st.write(f"í•™ìŠµ ì‹œê°„: {model_info['training_time']:.2f}ì´ˆ")
    else:
        st.info("ì•„ì§ í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    st.markdown("### ğŸ’¡ ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ")
    
    with st.expander("ì–´ë–¤ ëª¨ë¸ì„ ì„ íƒí• ê¹Œ?"):
        st.markdown("""
        **ì„ í˜• ëª¨ë¸ (Logistic/Linear)**
        - âœ… í•´ì„ì´ ì‰¬ì›€
        - âœ… ë¹ ë¥¸ í•™ìŠµ
        - âŒ ë¹„ì„ í˜• íŒ¨í„´ ëª»ì¡ìŒ
        
        **íŠ¸ë¦¬ ê¸°ë°˜ (RF, GB)**
        - âœ… ë¹„ì„ í˜• íŒ¨í„´
        - âœ… íŠ¹ì„± ì¤‘ìš”ë„
        - âŒ ê³¼ì í•© ìœ„í—˜
        
        **SVM**
        - âœ… ê³ ì°¨ì› ë°ì´í„°
        - âœ… ê°•ê±´í•œ ì„±ëŠ¥
        - âŒ ëŠë¦° í•™ìŠµ
        
        **ì‹ ê²½ë§**
        - âœ… ë³µì¡í•œ íŒ¨í„´
        - âœ… ë†’ì€ ì„±ëŠ¥
        - âŒ ë§ì€ ë°ì´í„° í•„ìš”
        """)
